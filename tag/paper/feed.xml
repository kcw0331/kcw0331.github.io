<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/paper/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2023-04-23T21:55:55+09:00</updated>
  <id>http://localhost:4000/tag/paper/feed.xml</id>

  
  
  

  
    <title type="html">꿈을 향해 나아가자! | </title>
  

  
    <subtitle>창우의 아이티블로그</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Paper - DeepSort 논문 리뷰</title>
      <link href="http://localhost:4000/tag/paper-paper(DeepSortReview)" rel="alternate" type="text/html" title="Paper - DeepSort 논문 리뷰" />
      <published>2023-02-21T09:40:00+09:00</published>
      <updated>2023-02-21T09:40:00+09:00</updated>
      <id>http://localhost:4000/tag/paper-paper(DeepSortReview)</id>
      <content type="html" xml:base="http://localhost:4000/tag/paper-paper(DeepSortReview)">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;Paper Review를 작성해 보았다. &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./paper-paper(RCNNReview)&quot;&gt;RCNN - Review&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./paper-paper(DeepSortReview)&quot;&gt;DeepSort 논문 리뷰&lt;/a&gt;&lt;/li&gt;
&lt;!--    &lt;li&gt;&lt;a href=&quot;./msai-1weeks(3)&quot;&gt;Msai - 1주차(3)&lt;/a&gt;&lt;/li&gt;--&gt;
&lt;!--    &lt;li&gt;&lt;a href=&quot;./msai-pokerimageclassification&quot;&gt;Msai - 포커이미지 분류&lt;/a&gt;&lt;/li&gt;--&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;deepsort-paper-review&quot;&gt;DeepSort Paper Review&lt;/h1&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;논문 리뷰를 작성해보았는데, 중점적인 내용을 요약 형식으로 작성을 하게 되었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;ABSTRACT&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Simple Online and Realtime Tracking (SORT) is a pragmatic
approach to multiple object tracking with a focus on simple,
effective algorithms.&lt;/li&gt;
  &lt;li&gt;In this paper, we integrate appearance
information to improve the performance of SORT.&lt;/li&gt;
  &lt;li&gt;Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity
switches&lt;/li&gt;
  &lt;li&gt;During online application, we establish measurement-to-track associations using
nearest neighbor queries in visual appearance space.&lt;/li&gt;
  &lt;li&gt;Experimental evaluation shows that our extensions reduce the number of identity switches by &lt;strong&gt;45%&lt;/strong&gt;, achieving overall competitive performance at high frame rates&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. INTRODUCTION&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While achieving overall good performance in terms of tracking precision and accuracy, SORT returns a relatively high number of identity switches. This is, because the employed association metric is only accurate when state estimation uncertainty is low.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overcome this issue by replacing the association metric with a more informed metric
that combines &lt;strong&gt;motion&lt;/strong&gt; and &lt;strong&gt;appearance information&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-sort-with-deep-association-metric&quot;&gt;2. SORT WITH DEEP ASSOCIATION METRIC&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Adopt a conventional single hypothesis tracking methodology with recursive Kalman filtering and frame-by-frame data association.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;21-track-handling-and-state-estimation&quot;&gt;2.1. Track Handling and State Estimation&lt;/h3&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;The track handling and Kalman filtering framework is mostly identical to the original formulation in SORT.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22--assignment-problem&quot;&gt;2.2  Assignment Problem&lt;/h3&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Into this problem formulation we integrate &lt;strong&gt;motion&lt;/strong&gt; and &lt;strong&gt;appearance information&lt;/strong&gt; through combination of two appropriate metrics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mahalanobis-distance&quot;&gt;Mahalanobis distance&lt;/h4&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;To incorporate motion information we use the (squared) &lt;strong&gt;Mahalanobis distance&lt;/strong&gt; between predicted Kalman states and newly arrived measurements&lt;/li&gt;
&lt;/ul&gt;

\[d^{(1)}(i, j) = (d_j - y_i)^T S_i^{-1} (d_j - y_i)\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;where Denote the projection of the i-th track distribution into measurement space by $(y_i ,S_i)$ and the $j$-th bounding
box detection by $d_j$ .&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;Mahalanobis distance&lt;/strong&gt; takes state
estimation uncertainty into account by measuring how many standard deviations the detection is away from the mean track
location.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using this metric it is possible to exclude
unlikely associations by thresholding the &lt;strong&gt;Mahalanobis distance&lt;/strong&gt; at a 95% confidence interval computed from the inverse
$χ^{2}$ distribution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[b^{(1)}_{i, j} = 1[d^{(1)}_{(i, j)} \le t^{(1)}]\]

&lt;ul&gt;
  &lt;li&gt;Denote this decision with an indicator that evaluates to 1 if the association between the $i$-th track and $j$-th detection is admissible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;smallest-cosine-distance&quot;&gt;Smallest cosine distance&lt;/h4&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;second metric measures the smallest cosine distance between the $i$-th track and $j$-th
detection in appearance space.&lt;/li&gt;
&lt;/ul&gt;

\[d^{(2)}_{(i, j)} = {min\{1 - r_{j}^{T} r_{k}^{(i)} | r_{k}^{(i)} \in R_{i}\}}\]

\[b_{i, j}^{(2)} = 1[d^{(2)}(i, j) \le t^{(2)}]\]

&lt;ul&gt;
  &lt;li&gt;Apply a pre-trained CNN to compute bounding box appearance descriptors.&lt;/li&gt;
&lt;/ul&gt;

\[c_{i, j} = \lambda d^{(1)}(i, j) + (1 - \lambda ) d^{(2)} (i, j)\]

&lt;ul&gt;
  &lt;li&gt;where we call an association admissible if it is within the gating region of both metrics.&lt;/li&gt;
&lt;/ul&gt;

\[b_{i, j} = \prod_{m = 1}^{2} b_{i, j}^{(m)}\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To build the association problem we combine both metrics using a weighted sum.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During our experiments we found that setting $ \lambda $ = 0 is a reasonable choice
when there is substantial camera motion.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;23-matching-cascade&quot;&gt;2.3 Matching Cascade&lt;/h3&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When an object is occluded for a longer period of time, subsequent Kalman filter predictions increase the uncertainty associated with the object location.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Therefore, we introduce a matching cascade that gives priority to more frequently seen
objects to encode our notion of probability spread in the association likelihood.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3--experiments&quot;&gt;3.  EXPERIMENTS&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This is a decrease of approximately &lt;strong&gt;45%&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, due to integration of appearance information we successfully maintain identities through longer occlusions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>kcw0331</name>
        
        
      </author>

      

      
        <category term="paper" />
      

      
        <summary type="html">Paper Review를 작성해 보았다. RCNN - Review DeepSort 논문 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Paper - R-CNN 논문 리뷰</title>
      <link href="http://localhost:4000/tag/paper-paper(RCNNReview)" rel="alternate" type="text/html" title="Paper - R-CNN 논문 리뷰" />
      <published>2023-01-19T09:40:00+09:00</published>
      <updated>2023-01-19T09:40:00+09:00</updated>
      <id>http://localhost:4000/tag/paper-paper(RCNNReview)</id>
      <content type="html" xml:base="http://localhost:4000/tag/paper-paper(RCNNReview)">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;Paper Review를 작성해 보았다. &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./paper-paper(RCNNReview)&quot;&gt;RCNN - Review&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./paper-paper(DeepSortReview)&quot;&gt;DeepSort 논문 리뷰&lt;/a&gt;&lt;/li&gt;
&lt;!--    &lt;li&gt;&lt;a href=&quot;./msai-1weeks(3)&quot;&gt;Msai - 1주차(3)&lt;/a&gt;&lt;/li&gt;--&gt;
&lt;!--    &lt;li&gt;&lt;a href=&quot;./msai-pokerimageclassification&quot;&gt;Msai - 포커이미지 분류&lt;/a&gt;&lt;/li&gt;--&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;r-cnn-review&quot;&gt;R-CNN Review&lt;/h1&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;R-CNN의 논문을 보면서 Two Stage와 One Stage에 대해 찾아보는 좋은 기회가 되었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://ganghee-lee.tistory.com/35&quot;&gt;참고 블로그 링크1&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://velog.io/@whiteamericano/R-CNN-%EC%9D%84-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90&quot;&gt;참고 블로그 링크2&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;1) classification
&lt;br /&gt;
   2) Object Detection
&lt;br /&gt;
   3) Image Segmentation
&lt;br /&gt;
   4) Visual Relationship&lt;/p&gt;

&lt;p&gt;CV(Computer Vision)은 위에 4가지로 분류가 된다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Classification : Single object에 대해서 object의 클래스를 분류하는 문제&lt;/li&gt;
  &lt;li&gt;Classification + Localization : Single Obect에 대해서 Object의 위치를 Bounding Box로 찾고 (Localization) + 클래스를 분류하는 것(Classification)&lt;/li&gt;
  &lt;li&gt;Object Detection : Multiple objects에서 각각의 object에 대해서 Classification + Localization을 수행하는 것&lt;/li&gt;
  &lt;li&gt;Instance Segmentation : Object Detection과 유사하지만, 다른점은 object의 위치를 bounding box가 아닌 실제 edge로 찾는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;11-2-stage-detector&quot;&gt;1.1. 2-Stage-Detector&lt;/h3&gt;

&lt;hr /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/2-stage-detector.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/2-stage-detector2.jpg&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;R-CNN계열(R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN etc.)&lt;/li&gt;
  &lt;li&gt;2-Stage Detector는 Region Proposal network등과 같은 알고리즘 및 네트워크를 통해 객체가 있을 영역(Region of Interest, RoI)을 추출해낸다. 이후 Classification과 Localization을 진행한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12-1-stage-detector&quot;&gt;1.2. 1-Stage-Detector&lt;/h3&gt;

&lt;hr /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/1-stage-detector.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/1-stage-detector2.jpg&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;YOLO, SSD계열(SSD, RetinaNet, RefineDet etc.)&lt;/li&gt;
  &lt;li&gt;1-Stage Detector는 RoI를 추출하지 않고 전체 이미지에 대해 Convolution Network로 Classification과 Localization을 진행한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://ganghee-lee.tistory.com/34&quot;&gt;참고 사진1&lt;/a&gt;
&lt;a href=&quot;https://ganghee-lee.tistory.com/35&quot;&gt;참고 사진2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;13-2-stage-detector와-1-stage-detector의-장단점&quot;&gt;1.3. 2-Stage-Detector와 1-Stage-Detector의 장단점&lt;/h3&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;2-Stage-Detector장점 &amp;amp; 단점
    &lt;ul&gt;
      &lt;li&gt;속도가 빠르지만 정확도가 떨어진다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1-Stage-Detector장점 &amp;amp; 단점
    &lt;ul&gt;
      &lt;li&gt;정확도가 높지만 속도가 느리다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-r-cnn&quot;&gt;2. R-CNN&lt;/h2&gt;

&lt;hr /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/R-CNN.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ganghee-lee.tistory.com/35&quot;&gt;참고 사진2&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;R-CNN은 위에서 보았던, 2-Stage-Detector라고 볼 수 있다.
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;R-CNN의 진행과정은 다음과 같다.
    &lt;ol&gt;
      &lt;li&gt;이미지 입력&lt;/li&gt;
      &lt;li&gt;Selective search알고리즘을 사용하여 대략 2,000개의 regional proposal output추출 후 동일한 input size를 만들기 위해 warp해준다.(마지막 FC layer에서 input size는 고정이기 때문에 Convolution Layer에 대한 output size도 동일해야 해서 input size에서 부터 동일한 사이즈를 넣어주어야 한다.)&lt;/li&gt;
      &lt;li&gt;대략 2,000개의 warped image를 CNN모델에 투입&lt;/li&gt;
      &lt;li&gt;각각의 Convolution 결과를 기반으로 Classification(SVM)을 진행 후 결과 추출&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;21-region-proposal&quot;&gt;2.1. Region Proposal&lt;/h3&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Sliding window(이미지의 맨 위에서 부터 쭉 읽으면서 탐색)은 탐색이 너무 느리고 비효율적이여서 Selective search알고리즘을 사용하게 됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;selective-search&quot;&gt;Selective search&lt;/h4&gt;

&lt;hr /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/selectivesearch.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ganghee-lee.tistory.com/35&quot;&gt;참고 사진2&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Selective search의 진행과정
    &lt;ol&gt;
      &lt;li&gt;색상, 질감, 영역크기 등을 이용하여 Non-object-based segmentation수행&lt;/li&gt;
      &lt;li&gt;Bottom-up 방식으로 small segmented areas들을 합쳐서 더 큰 segmented areas를 만듦&lt;/li&gt;
      &lt;li&gt;두번째를 반복을 통해 최종적으로 대략 2,000개의 region proposal을 생성&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;* 주의사항: CNN에 넣기 전에 같은 사이즈로 warp해주어야 한다.&lt;/p&gt;

&lt;h3 id=&quot;22-cnn&quot;&gt;2.2. CNN&lt;/h3&gt;

&lt;hr /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/alexnetblockdiagram.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96&quot;&gt;참고 사진3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CNN은 AlexNet의 구조를 사용하며 Resion proposal로 부터 4096차원의 특징 벡터를 뽑아낸 후 고정된 길이의 특징 벡터를 만들어낸다.&lt;/p&gt;

&lt;h3 id=&quot;23-svm&quot;&gt;2.3. SVM&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;softmax보다 SVM이 좋은 성능을 보여 SVM을 사용하였다고 한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/svm.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine#/media/File:SVM_margin.png&quot;&gt;참고 사진4&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SVM은 CNN(AlexNet)으로 부터 추출된 각각의 특징 벡터들의 점수를 클래스별로 계산 후 객체인지 아닌지, 어떤 객체인지 등을 판별하는 역할을 하는 Classifier이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-4-bounding-box-regression&quot;&gt;2-4). Bounding Box Regression&lt;/h3&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Bounding Box Regression은 처음에 y로 줬던 레이블과, CNN을 통과해서 나온 Bounding Box, 이 두 가지의 차이를 구해서 차이를 줄이도록 조정하는 선형회귀 모델 절차&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/built/images/paper/boundingbox.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://velog.io/@whiteamericano/R-CNN-%EC%9D%84-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90&quot;&gt;참고 사진5&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위 그림에서 P가 처음에 region proposal을 통해서 제안된 Bounding Box이고, G는 Ground Truth Bounding Box(정답 박스) 여기서 P를 정답 G에 맞추도록 transform 하는 것을 학습하는 것이 바로 Bounding Box Regression의 목표&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-정리&quot;&gt;3. 정리&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;이미지 하나의 임의의 영역들로 잘게 잘라서 merge 하고 warping 한 후,&lt;/li&gt;
  &lt;li&gt;CNN의 input으로 통과하여 Feature vecture를 추출하고,&lt;/li&gt;
  &lt;li&gt;적용된 label들을 이용해 SVM 분류와 Bounding Box Regression을 수행해서 영역 위치까지 예측을 하게 되는 모델이 바로 R-CNN&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>kcw0331</name>
        
        
      </author>

      

      
        <category term="paper" />
      

      
        <summary type="html">Paper Review를 작성해 보았다. RCNN - Review DeepSort 논문 리뷰</summary>
      

      
      
    </entry>
  
</feed>
